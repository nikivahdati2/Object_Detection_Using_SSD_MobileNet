{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2414edc0",
      "metadata": {
        "id": "2414edc0"
      },
      "source": [
        "\n",
        "# Object Detection using TensorFlow and Pascal VOC 2007 Dataset\n",
        "\n",
        "In this exercise, we will adapt our image classification task to an object detection task. Object detection involves not only classifying objects within an image but also localizing them with bounding boxes.\n",
        "\n",
        "Note: Due to the limited computational resources available, we'll be using a smaller subset of the Pascal VOC 2007 dataset and a lightweight object detection model. This might result in lower accuracy, but the focus of this exercise is on understanding the concepts and workflow of object detection.\n",
        "\n",
        "## Steps:\n",
        "1. Install (if necessary) and Import the libraries you will need for this project\n",
        "2. Load the Pascal VOC 2007 dataset\n",
        "3. Use a pre-trained object detection model (SSD MobileNet V2)\n",
        "4. Display detected objects with bounding boxes\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87J_3TzSGdU",
        "outputId": "21b8d09b-5642-46ba-ec72-bab7487ed42a"
      },
      "id": "d87J_3TzSGdU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29112b7e",
      "metadata": {
        "id": "29112b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79019d08-e587-4fec-886d-f61f728c2afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (16.1.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.5)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.20.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow tensorflow-hub tensorflow-datasets matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  print(\"GPU is not available, USING CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e10x9SXSo69",
        "outputId": "a88f4737-552b-43b2-cbc4-40a42862bd20"
      },
      "id": "7e10x9SXSo69",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99553a86",
      "metadata": {
        "id": "99553a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b100180e-500d-4cc4-c929-af11904bcbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.0\n",
            "TensorFlow Hub version: 0.16.1\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow Hub version:\", hub.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad5ebb2",
      "metadata": {
        "id": "9ad5ebb2"
      },
      "source": [
        "\n",
        "### Load the VOC2007 dataset\n",
        "\n",
        "We will use the VOC2007 dataset, which contains images with annotations for object detection. For demonstration purposes, we will load a small subset of the dataset using TensorFlow Datasets.\n",
        "\n",
        "- VOC2007 is a dataset for object detection, segmentation, and image classification.\n",
        "- We define a function load_data to load the COCO dataset.\n",
        "- tfds.load is a function that downloads and prepares the dataset.\n",
        "- We use only 1% of the training data to keep the demonstration manageable.\n",
        "- shuffle_files=True ensures that we get a random sample of the dataset.\n",
        "- with_info=True returns additional information about the dataset, which we'll use later.\n",
        "\n",
        "- The PASCAL VOC2007 (Visual Object Classes) dataset is a widely used benchmark dataset for object recognition tasks in computer vision. It comprises a collection of images annotated with bounding boxes and class labels for objects belonging to 20 different categories.\n",
        "\n",
        "Key characteristics of the VOC2007 dataset:\n",
        "\n",
        "- Purpose: Primarily used for training and evaluating object detection algorithms, but also applicable to other tasks like image classification and semantic segmentation.\n",
        "- Object Categories: Includes a diverse set of 20 object classes, ranging from people and animals to vehicles and indoor items.\n",
        "- Data Format: The dataset provides images along with corresponding annotation files containing bounding box coordinates and class labels for each object in the image.\n",
        "- Image Variety: Features a wide range of images captured in diverse real-world scenarios, offering realistic challenges for object recognition models.\n",
        "- Benchmark: Serves as a standard benchmark for comparing the performance of different object detection algorithms, fostering progress in the field.\n",
        "\n",
        "Common use cases of the VOC2007 dataset:\n",
        "- Training: Used as training data to teach object detection models to identify and localize objects within images.\n",
        "- Evaluation: Employed to evaluate the performance of trained models by comparing their predictions against the ground truth annotations.\n",
        "- Research: Utilized in research to develop and test new object detection algorithms and techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca53d8ae",
      "metadata": {
        "id": "ca53d8ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "cb9af65eba574df6800ec1f2eef0ee48",
            "950a656f9df04e1e9c668ef26273d8ea",
            "3fec034040724ac5b4f4aea34022529d",
            "1085d4bcb4184bd5a67c10a20d1d7ac3",
            "2af5d29d158541fe8bff198c596cb515",
            "5ae86c57974349e797050a6c376a54e1",
            "4bce7fa03b64483b8aec6f03b7cb2735",
            "655502fb46f041fbb6a986546def9369",
            "73e7c78bd6804e18bebb0f0ef20e94f9",
            "78ad7357460e428d900bb564241412c4",
            "e20c13e64e0f41a3887ed4b04e63512a",
            "e39bec197dd94d599680a08d066e8786",
            "d2d392a7aefb4a4ba6c2faa07b4b0a36",
            "7bf0cbaf547c437588058951e44a4f71",
            "8c334a5ce5954b108d73f6105a00991e",
            "88b1cdbb53b84c79878392be31eae20c",
            "a4c3ac3006ed47f4a743c256d8b3e0f7",
            "2fa102e50fda43db85da17e31a49149e",
            "39872348f62a488c87d789434c7e61f8",
            "ffbcd42f193e4d0790f87c77f461bb14",
            "8aa698579cae45429660a4bc316c4a5c",
            "99795bca23274ad2a186162ad117993a",
            "f480fd50b88c4a65b89727a3ea8e1087",
            "62e6e3f7afa04db398b7445354804557",
            "fd7502b47cc24f2794f7c7e3a0e99331",
            "53b0ea30a30a44bbba29db19dc2216bc",
            "0508ebc903b34277bc2a3797c6a44a2e",
            "0078471249bf40808d6e8a5e765a9f09",
            "73a68ecaf65c411f9c16e57f18e90639",
            "a71919d2157c40fc88f4a7fbc3506157",
            "14b235699da04c6b87ff0355e4f5bec2",
            "67997a6f7ed349a883427c6423b3fc3d",
            "95a50b22991c408a90bafdfe3270075b"
          ]
        },
        "outputId": "8a16d60a-13be-41e3-8aff-7ffa7085a10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 868.85 MiB (download: 868.85 MiB, generated: Unknown size, total: 868.85 MiB) to /root/tensorflow_datasets/voc/2007/4.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb9af65eba574df6800ec1f2eef0ee48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e39bec197dd94d599680a08d066e8786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f480fd50b88c4a65b89727a3ea8e1087"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a smaller dataset\n",
        "def load_data(split='train'):\n",
        "    dataset, info = tfds.load('voc/2007', split=split, shuffle_files=True, with_info=True)\n",
        "    return dataset, info\n",
        "\n",
        "# Load the train dataset and extract info\n",
        "train_dataset, train_info = load_data('train[:10%]')\n",
        "\n",
        "# Load the validation dataset\n",
        "validation_dataset, validation_info = load_data('validation[:10%]')\n",
        "\n",
        "# Get class names\n",
        "class_names = train_info.features[\"objects\"][\"label\"].names  # Changed from ds_info to train_info\n",
        "print(\"Class names:\", class_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AH_X2KQ1UXac"
      },
      "id": "AH_X2KQ1UXac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394a8233",
      "metadata": {
        "id": "394a8233"
      },
      "outputs": [],
      "source": [
        "def display_examples(dataset, n=3):  # Display 'n' examples by default\n",
        "    for example in dataset.take(n):\n",
        "        image = example[\"image\"]\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"Image with Ground Truth Bounding Boxes\")\n",
        "\n",
        "        # Draw ground truth boxes\n",
        "        for box in example[\"objects\"][\"bbox\"]:\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            rect = patches.Rectangle((xmin * image.shape[1], ymin * image.shape[0]),\n",
        "                                    (xmax - xmin) * image.shape[1], (ymax - ymin) * image.shape[0],\n",
        "                                    linewidth=1, edgecolor='g', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "display_examples(train_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd8c93b",
      "metadata": {
        "id": "0cd8c93b"
      },
      "source": [
        "### Find Images with Specific Classes\n",
        "\n",
        "We got  the list of all class names in the VOC2007 dataset and select images containing our target classes (e.g., person, car, bird).\n",
        "\n",
        "- `class_names` provides the list of class names.\n",
        "- `target_class_ids` contains the IDs of the classes we are interested in.\n",
        "- `find_images_with_classes` is a function to find images containing our target classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea1b5f4e",
      "metadata": {
        "id": "ea1b5f4e"
      },
      "source": [
        "### When To Load the model\n",
        "Loading the model early (right after dataset loading):\n",
        "\n",
        "Pros: Model is immediately available; clear separation of setup and processing.\n",
        "Cons: Potentially inefficient if data prep is extensive or fails.\n",
        "\n",
        "\n",
        "Loading the model after data preparation:\n",
        "\n",
        "Pros: More efficient resource use; avoids unnecessary loading if data prep fails.\n",
        "Cons: Model isn't available for any data prep steps that might need it.\n",
        "\n",
        "\n",
        "In our specific case, loading the model after data preparation is slightly better because:\n",
        "\n",
        "Our data prep doesn't need the model.\n",
        "It's more resource-efficient.\n",
        "It follows a logical flow: prepare data, load tools, process data.\n",
        "It avoids unnecessary model loading if data prep fails.\n",
        "\n",
        "However, the difference is minimal in this small-scale example. For beginners, loading major components upfront can sometimes be clearer and easier to follow.\n",
        "As a best practice, aim to load your model as close as possible to where you'll use it, ensuring all necessary data and resources are ready first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12b49aa",
      "metadata": {
        "id": "b12b49aa"
      },
      "outputs": [],
      "source": [
        "#Load a pre-trained object detection model\n",
        "detector = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ae757a",
      "metadata": {
        "id": "d3ae757a"
      },
      "source": [
        "Let's break this down:\n",
        "\n",
        "- 1. hub.load(): This function is from TensorFlow Hub (tensorflow_hub). It downloads and loads models from the TensorFlow Hub repository.\n",
        "- 2. \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\": This is the URL of the specific model we're loading. It's an SSD (Single Shot Detector) MobileNet V2 model, which is efficient for object detection tasks.\n",
        "- 3. Detector: The loaded model is assigned to this variable. It becomes a callable object that you can use for object detection.\n",
        "\n",
        "Advantages of this approach:\n",
        "\n",
        "Concise and readable\n",
        "Directly loads the model without additional wrapper functions\n",
        "TensorFlow Hub handles caching, so subsequent loads will be faster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86360a19",
      "metadata": {
        "id": "86360a19"
      },
      "source": [
        "### Display Detected Objects with Bounding Boxes\n",
        "\n",
        "We will use the pre-trained model to detect objects in our selected images and display them with bounding boxes.\n",
        "\n",
        "- `detector` is the pre-trained object detection model.\n",
        "- `detect_objects` is a function that uses the model to detect objects in an image.\n",
        "- `display_detections` is a function to display the detected objects with bounding boxes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396c0a53",
      "metadata": {
        "id": "396c0a53"
      },
      "source": [
        "\n",
        "### Helper Function to Display Bounding Boxes on Images\n",
        "\n",
        "The `display_image_with_boxes` function takes an image, bounding boxes, and class names, then displays the image with bounding boxes drawn around detected objects.\n",
        "- run_detector: This function prepares an image and runs it through our object detection model.\n",
        "- plot_detections: This function visualizes the detected objects by drawing bounding boxes and labels on the image.\n",
        "\n",
        " process_uploaded_image which processes an uploaded image for object detection. The function takes the raw image data as input, preprocesses the image, runs the object detection model, and then plots and prints the detected objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fc8152",
      "metadata": {
        "id": "d0fc8152"
      },
      "outputs": [],
      "source": [
        "# Run Detector and Visualize\n",
        "def run_detector_and_visualize(example):\n",
        "    image = example[\"image\"]\n",
        "    ground_truth_boxes = example[\"objects\"][\"bbox\"]\n",
        "\n",
        "    # Preprocess and run detection\n",
        "    converted_img = tf.image.convert_image_dtype(image, tf.uint8)[tf.newaxis, ...]\n",
        "    result = detector(converted_img)\n",
        "    result = {key: value.numpy() for key, value in result.items()}\n",
        "\n",
        "    # Visualize results (with ground truth for comparison)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(image)\n",
        "\n",
        "    # Ground truth boxes (VOC format is [xmin, ymin, xmax, ymax])\n",
        "    for box in ground_truth_boxes:\n",
        "        ymin, xmin, ymax, xmax = box\n",
        "        rect = patches.Rectangle((xmin * image.shape[1], ymin * image.shape[0]),\n",
        "                                (xmax - xmin) * image.shape[1], (ymax - ymin) * image.shape[0],\n",
        "                                linewidth=1, edgecolor='g', facecolor='none', label='Ground Truth')\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "    # Predicted boxes\n",
        "    for i, score in enumerate(result['detection_scores'][0]):\n",
        "        if score > 0.5:  # Confidence threshold\n",
        "            ymin, xmin, ymax, xmax = result['detection_boxes'][0][i]\n",
        "            class_id = int(result['detection_classes'][0][i])\n",
        "\n",
        "            # Handle invalid class IDs (classes outside the VOC dataset)\n",
        "            if class_id < len(class_names):\n",
        "                label = class_names[class_id]\n",
        "\n",
        "            rect = patches.Rectangle((xmin * image.shape[1], ymin * image.shape[0]),\n",
        "                                    (xmax - xmin) * image.shape[1], (ymax - ymin) * image.shape[0],\n",
        "                                    linewidth=1, edgecolor='r', facecolor='none', label='Predicted')\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "            # Moved plt.text to the correct loop for the predicted box\n",
        "            plt.text(xmin * image.shape[1], ymin * image.shape[0] - 5, f'{label}: {score:.2f}', color='white', backgroundcolor='r')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565b6ed3",
      "metadata": {
        "id": "565b6ed3"
      },
      "source": [
        "###   Process and Display Images with Detections\n",
        "The `detect_and_display` function runs object detection on an image and displays the results, as you saw above. The function converts the image to the appropriate format, runs the detector, and then uses the helper function to display the results.\n",
        "\n",
        " process_uploaded_image which processes an uploaded image for object detection. The function takes the raw image data as input, preprocesses the image, runs the object detection model, and then plots and prints the detected objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e937b19",
      "metadata": {
        "collapsed": true,
        "id": "6e937b19"
      },
      "outputs": [],
      "source": [
        "# take a few examples from the training set\n",
        "for example in train_dataset.take(2):  # Process 2 images\n",
        "    run_detector_and_visualize(example)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wXOchOCE9c5n",
      "metadata": {
        "id": "wXOchOCE9c5n"
      },
      "source": [
        "#### Your Turn\n",
        "Process a few images from the dataset\n",
        "print(\"\\nProcessing sample images from the dataset:\") for i, example in enumerate(train_dataset.take(3)): print(f\"\\nSample image {i+1}\") image = example['image'].numpy() detections = run_detector(detector, image) plot_detections(image, detections, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_detections(image_np, detections, class_names):\n",
        "    \"\"\"\n",
        "    Plots object detections on an image.\n",
        "\n",
        "    Args:\n",
        "        image_np: A NumPy array representing the image.\n",
        "        detections: A dictionary containing detection results.\n",
        "        class_names: A list of class names.\n",
        "    \"\"\"\n",
        "\n",
        "    image_np_with_detections = image_np.copy()\n",
        "    height, width, _ = image_np.shape\n",
        "\n",
        "    for i in range(detections['detection_boxes'].shape[0]):\n",
        "        if detections['detection_scores'][i] > 0.5:  # Confidence threshold\n",
        "            ymin, xmin, ymax, xmax = detections['detection_boxes'][i]\n",
        "\n",
        "            # Convert normalized coordinates to pixel coordinates\n",
        "            (xmin, xmax, ymin, ymax) = (xmin * width, xmax * width,\n",
        "                                        ymin * height, ymax * height)\n",
        "\n",
        "            # Draw bounding box\n",
        "            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin),\n",
        "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "            # Add label and score\n",
        "            class_id = int(detections['detection_classes'][i])\n",
        "            if class_id < len(class_names):\n",
        "                label = class_names[class_id]\n",
        "                score = detections['detection_scores'][i]\n",
        "                plt.text(xmin, ymin - 5, f'{label}: {score:.2f}', color='white',\n",
        "                         backgroundcolor='r')\n",
        "\n",
        "    plt.figure(figsize=(12, 16))\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    plt.title('Object Detections')\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lTca4HMib6uF"
      },
      "id": "lTca4HMib6uF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"/nProcessing sample image from the dataset:\")\n",
        "for i, example in enumerate(train_dataset.take(3)):\n",
        "  print(f\"\\nSample image {i+1}\")\n",
        "  image = example['image'].numpy()\n",
        "  detections = run_detector(detector, image)\n",
        "\n",
        "  # Check if detections is None before proceeding\n",
        "  if detections is not None:\n",
        "    plot_detections(image, detections, class_names)\n",
        "  else:\n",
        "    print(\"No detections found for this image.\")"
      ],
      "metadata": {
        "id": "fF_M7-qnbDJU"
      },
      "id": "fF_M7-qnbDJU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cafd587f",
      "metadata": {
        "id": "cafd587f"
      },
      "source": [
        "## Mode Evaluation\n",
        "###  Define the Evaluation Function\n",
        "\n",
        " The function called evaluate_model_performance which evaluates the performance of our object detection model on a dataset. The function takes three arguments: the dataset to evaluate on, the object detection model, and the number of images to use for evaluation. It calculates and prints the accuracy of the model based on the detections.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24dc1ecf",
      "metadata": {
        "id": "24dc1ecf"
      },
      "outputs": [],
      "source": [
        "#Evaluate Model Performance\n",
        "def evaluate_model_performance(dataset, detector, iou_threshold=0.5, num_samples=100):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    for example in dataset.take(num_samples):\n",
        "        image = example[\"image\"].numpy()\n",
        "        gt_boxes = example[\"objects\"][\"bbox\"].numpy()\n",
        "        gt_labels = example[\"objects\"][\"label\"].numpy()\n",
        "\n",
        "        # Preprocess and run detection (same as before)\n",
        "        converted_img = tf.image.convert_image_dtype(image, tf.uint8)[tf.newaxis, ...]\n",
        "        result = detector(converted_img)\n",
        "        result = {key: value.numpy() for key, value in result.items()}\n",
        "        pred_boxes = result['detection_boxes'][0]\n",
        "        pred_scores = result['detection_scores'][0]\n",
        "        pred_labels = result['detection_classes'][0].astype(int)\n",
        "\n",
        "        # Iterate over predicted boxes\n",
        "        for i, score in enumerate(pred_scores):\n",
        "            if score < 0.5:  # Confidence threshold\n",
        "                continue\n",
        "\n",
        "            # Convert box coordinates to [ymin, xmin, ymax, xmax]\n",
        "            pred_box = pred_boxes[i]\n",
        "            pred_box = [pred_box[1], pred_box[0], pred_box[3], pred_box[2]]\n",
        "\n",
        "            # Find matching ground truth box (if any) based on IoU\n",
        "            best_iou = 0\n",
        "            for j, gt_box in enumerate(gt_boxes):\n",
        "                iou = calculate_iou(gt_box, pred_box)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    gt_index = j\n",
        "\n",
        "            # If IoU exceeds threshold, check class match\n",
        "            if best_iou > iou_threshold:\n",
        "                if pred_labels[i] == gt_labels[gt_index]:\n",
        "                    true_positives += 1\n",
        "                else:\n",
        "                    false_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "        # Count false negatives (missed ground truth boxes)\n",
        "        false_negatives += len(gt_boxes) - true_positives\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "\n",
        "    print(f\"Model Performance (IoU Threshold = {iou_threshold:.2f}):\")\n",
        "    print(f\"True Positives: {true_positives}\")\n",
        "    print(f\"False Positives: {false_positives}\")\n",
        "    print(f\"False Negatives: {false_negatives}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# (You'll need to implement a 'calculate_iou' function)\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"Calculates the Intersection over Union (IoU) between two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1 (list): Coordinates of the first box in the format [ymin, xmin, ymax, xmax].\n",
        "        box2 (list): Coordinates of the second box in the same format.\n",
        "\n",
        "    Returns:\n",
        "        float: The IoU value (between 0 and 1).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate coordinates of the intersection rectangle\n",
        "    y1 = max(box1[0], box2[0])\n",
        "    x1 = max(box1[1], box2[1])\n",
        "    y2 = min(box1[2], box2[2])\n",
        "    x2 = min(box1[3], box2[3])\n",
        "\n",
        "    # 2. Calculate areas of the intersection and the union\n",
        "    intersection_area = max(0, y2 - y1) * max(0, x2 - x1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    # 3. Calculate IoU\n",
        "    if union_area == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    else:\n",
        "        iou = intersection_area / union_area\n",
        "        return iou\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Evaluating model performance...\")\n",
        "evaluate_model_performance(validation_dataset, detector)  # Use test data for evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be74ea3",
      "metadata": {
        "id": "9be74ea3"
      },
      "source": [
        "### Upload your Image\n",
        "This final block allows you to input your own image URL for object detection, making the exercise interactive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8786f0d",
      "metadata": {
        "id": "d8786f0d"
      },
      "source": [
        "### Object Detection Evaluation Core Concepts\n",
        "\n",
        "* Object detection models need to be evaluated on two fronts:\n",
        "\n",
        "- Classification Accuracy: Did the model correctly identify the object's class (e.g., person, car, bird)?\n",
        "- Localization Accuracy: Did the model accurately draw a bounding box around the object?\n",
        "\n",
        " Our exercise  focuses on assessing localization accuracy using the Intersection over Union (IoU) metric.\n",
        "\n",
        "* Understanding IoU (Intersection over Union)\n",
        "\n",
        "IoU measures how much two bounding boxes overlap.\n",
        "\n",
        "- A perfect match (predicted box perfectly matches the ground truth box) has an IoU of 1.\n",
        "- No overlap has an IoU of 0.\n",
        "\n",
        "The iou_threshold in the code (default 0.5) means a predicted box is considered a \"true positive\" only if its IoU with a ground truth box is 0.5 or higher.\n",
        "\n",
        "* Output Interpretation:\n",
        "\n",
        "The function will print the following metrics:\n",
        "\n",
        "- True Positives (TP): The number of detected objects where both the class label and bounding box are correct (IoU above the threshold).\n",
        "- False Positives (FP): The number of detected objects that are either misclassified or have an IoU below the threshold.\n",
        "- False Negatives (FN): The number of ground truth objects that the model missed entirely.\n",
        "- Precision: The proportion of positive detections that were actually correct (TP / (TP + FP)). A high precision means the model makes few false alarms.\n",
        "- Recall: The proportion of actual positive objects that the model successfully detected (TP / (TP + FN)). A high recall means the model misses few objects.\n",
        "\n",
        "Example Results:\n",
        "Let's say the output is:\n",
        "\n",
        "Model Performance (IoU Threshold = 0.50):\n",
        "True Positives: 75\n",
        "False Positives: 20\n",
        "False Negatives: 15\n",
        "Precision: 0.79\n",
        "Recall: 0.83\n",
        "Interpretation:\n",
        "\n",
        "- The model correctly detected and localized 75 objects.\n",
        "- It made 20 incorrect detections (wrong class or poor box placement).\n",
        "- It missed 15 objects that were actually present in the images.\n",
        "- Precision is 0.79, meaning 79% of the model's positive detections were accurate.\n",
        "- Recall is 0.83, meaning the model found 83% of the actual objects in the images.\n",
        "\n",
        "* Key Takeaways:\n",
        "- Precision vs. Recall: There's often a trade-off between these two. Increasing the confidence threshold (e.g., to 0.6) might improve    precision (fewer false alarms) but likely lower recall (more missed objects).\n",
        "- IoU Threshold: The choice of IoU threshold significantly impacts the results. A higher threshold makes the evaluation stricter, potentially lowering both precision and recall.\n",
        "- Limitations: This evaluation only covers a limited number of samples (num_samples). For a more comprehensive assessment, you'd ideally use a larger and more diverse evaluation set.\n",
        "- Single Metric: Precision and recall alone don't tell the whole story. Consider using other metrics like F1 score (harmonic mean of precision and recall) for a more balanced view of performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06664892",
      "metadata": {
        "id": "06664892"
      },
      "source": [
        " ### Instructions  to Upload Your Own Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "oMokp0C_7NJb",
      "metadata": {
        "id": "oMokp0C_7NJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6cf4e1-ab0d-4122-dcc4-bc4c69a1ade1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To upload your own image for object detection:\n",
            "1. If using Google Colab, use:\n",
            "   from google.colab import files\n",
            "   uploaded = files.upload()\n",
            "   image_data = next(iter(uploaded.values()))\n",
            "2. Then run:\n",
            "   process_uploaded_image(image_data)\n"
          ]
        }
      ],
      "source": [
        "# Function to process uploaded images (for Google Colab)\n",
        "def process_uploaded_image(image_data):\n",
        "    \"\"\"Processes and displays detections for an uploaded image.\"\"\"\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "    image_np = np.array(image)  # Convert PIL Image to NumPy array\n",
        "    detections = run_detector(detector, image_np)\n",
        "    plot_detections_with_heatmap(image_np, detections, class_names)\n",
        "\n",
        "    # Print detected objects (example)\n",
        "    print(\"Detected objects:\")\n",
        "    for i, score in enumerate(detections['detection_scores'][0]):\n",
        "        if score > 0.5:  # Confidence threshold\n",
        "            class_id = int(detections['detection_classes'][0][i])\n",
        "            label = class_names[class_id] if class_id < len(class_names) else \"UNKNOWN\"\n",
        "            print(f\"- {label} with confidence {score:.2f}\")\n",
        "\n",
        "# Instructions for image uploading (if in Google Colab)\n",
        "print(\"\\nTo upload your own image for object detection:\")\n",
        "print(\"1. If using Google Colab, use:\")\n",
        "print(\"   from google.colab import files\")\n",
        "print(\"   uploaded = files.upload()\")\n",
        "print(\"   image_data = next(iter(uploaded.values()))\")\n",
        "print(\"2. Then run:\")\n",
        "print(\"   process_uploaded_image(image_data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0582fe7",
      "metadata": {
        "id": "b0582fe7"
      },
      "source": [
        "### Conclusion\n",
        "This exercise introduces you to object detection while keeping computational requirements relatively low. It uses a pre-trained model, so no training is required, making it suitable for systems with limited resources.\n",
        "\n",
        "Using pre-trained models for complex tasks\n",
        "The basics of object detection (bounding boxes, class labels, confidence scores)\n",
        "Visualizing detection results\n",
        "Simple analysis of detection outputs\n",
        "\n",
        "The exercise is also interactive, allowing students to try object detection on their own chosen images. Copy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf0aa9f",
      "metadata": {
        "id": "faf0aa9f"
      },
      "source": [
        "## Questions for Reflection and Analysis:\n",
        "1. **Conceptual Understanding:**\n",
        "- What is the main difference between image classification and object detection? How is this difference evident in the output of this exercise?\n",
        "- Explain why we chose the SSD MobileNet V2 model for this task. What are its advantages and limitations, especially in the context of limited computational resources?\n",
        "\n",
        "2. **Code Interpretation:**\n",
        "- Describe the role of the find_images_with_classes function. Why is it useful when working with a large dataset like COCO?\n",
        "- In the plot_detections function, how does the threshold value (threshold=0.5) impact the number of objects displayed?\n",
        "- Explain how the heatmap visualization helps you understand the model's confidence in its detections.\n",
        "\n",
        "3. **Observing Results and Limitations:**\n",
        "- Run the exercise multiple times. Which types of objects does the model tend to detect more accurately? Which ones are more challenging? Can you explain why?\n",
        "- Observe the bounding boxes. Are there any instances where the boxes are inaccurate or miss the object entirely? What factors in the images might be contributing to these errors?\n",
        "- How would you expect the accuracy of the model to change if we had used the entire Pascal VOC 2007 dataset instead of a small subset? Why?\n",
        "\n",
        "4. **Critical Thinking:**\n",
        "- How could you modify the code to detect a specific set of objects, like only animals or only vehicles?\n",
        "- If you wanted to train your own object detection model, what steps would you need to take? What are some challenges you might encounter?\n",
        "- Given the limitations of this model, in what real-world scenarios might it still be useful for object detection?\n",
        "\n",
        "5. **Going Further (Optional):** (Bonus points)\n",
        "- Research other object detection models available in TensorFlow Hub. Compare and contrast them with SSD MobileNet V2 in terms of accuracy, speed, and resource requirements.\n",
        "- Try running a few images through a more powerful object detection model online (if available). Compare the results to the output of this exercise. What differences do you notice?\n",
        "- Important: Remember, the goal here isn't perfect accuracy. It's to understand the core concepts of object detection, the limitations of working with restricted resources, and how to critically analyze the results.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb9af65eba574df6800ec1f2eef0ee48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_950a656f9df04e1e9c668ef26273d8ea",
              "IPY_MODEL_3fec034040724ac5b4f4aea34022529d",
              "IPY_MODEL_1085d4bcb4184bd5a67c10a20d1d7ac3"
            ],
            "layout": "IPY_MODEL_2af5d29d158541fe8bff198c596cb515"
          }
        },
        "950a656f9df04e1e9c668ef26273d8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae86c57974349e797050a6c376a54e1",
            "placeholder": "",
            "style": "IPY_MODEL_4bce7fa03b64483b8aec6f03b7cb2735",
            "value": "DlCompleted...:100%"
          }
        },
        "3fec034040724ac5b4f4aea34022529d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_655502fb46f041fbb6a986546def9369",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73e7c78bd6804e18bebb0f0ef20e94f9",
            "value": 1
          }
        },
        "1085d4bcb4184bd5a67c10a20d1d7ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ad7357460e428d900bb564241412c4",
            "placeholder": "",
            "style": "IPY_MODEL_e20c13e64e0f41a3887ed4b04e63512a",
            "value": "2/2[01:53&lt;00:00,19.47s/url]"
          }
        },
        "2af5d29d158541fe8bff198c596cb515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae86c57974349e797050a6c376a54e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bce7fa03b64483b8aec6f03b7cb2735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "655502fb46f041fbb6a986546def9369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "73e7c78bd6804e18bebb0f0ef20e94f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78ad7357460e428d900bb564241412c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e20c13e64e0f41a3887ed4b04e63512a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e39bec197dd94d599680a08d066e8786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2d392a7aefb4a4ba6c2faa07b4b0a36",
              "IPY_MODEL_7bf0cbaf547c437588058951e44a4f71",
              "IPY_MODEL_8c334a5ce5954b108d73f6105a00991e"
            ],
            "layout": "IPY_MODEL_88b1cdbb53b84c79878392be31eae20c"
          }
        },
        "d2d392a7aefb4a4ba6c2faa07b4b0a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c3ac3006ed47f4a743c256d8b3e0f7",
            "placeholder": "",
            "style": "IPY_MODEL_2fa102e50fda43db85da17e31a49149e",
            "value": "DlSize...:100%"
          }
        },
        "7bf0cbaf547c437588058951e44a4f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39872348f62a488c87d789434c7e61f8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffbcd42f193e4d0790f87c77f461bb14",
            "value": 1
          }
        },
        "8c334a5ce5954b108d73f6105a00991e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa698579cae45429660a4bc316c4a5c",
            "placeholder": "",
            "style": "IPY_MODEL_99795bca23274ad2a186162ad117993a",
            "value": "868/868[01:53&lt;00:00,11.26MiB/s]"
          }
        },
        "88b1cdbb53b84c79878392be31eae20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c3ac3006ed47f4a743c256d8b3e0f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa102e50fda43db85da17e31a49149e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39872348f62a488c87d789434c7e61f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ffbcd42f193e4d0790f87c77f461bb14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8aa698579cae45429660a4bc316c4a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99795bca23274ad2a186162ad117993a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f480fd50b88c4a65b89727a3ea8e1087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62e6e3f7afa04db398b7445354804557",
              "IPY_MODEL_fd7502b47cc24f2794f7c7e3a0e99331",
              "IPY_MODEL_53b0ea30a30a44bbba29db19dc2216bc"
            ],
            "layout": "IPY_MODEL_0508ebc903b34277bc2a3797c6a44a2e"
          }
        },
        "62e6e3f7afa04db398b7445354804557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0078471249bf40808d6e8a5e765a9f09",
            "placeholder": "",
            "style": "IPY_MODEL_73a68ecaf65c411f9c16e57f18e90639",
            "value": "Extractioncompleted...:100%"
          }
        },
        "fd7502b47cc24f2794f7c7e3a0e99331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71919d2157c40fc88f4a7fbc3506157",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14b235699da04c6b87ff0355e4f5bec2",
            "value": 1
          }
        },
        "53b0ea30a30a44bbba29db19dc2216bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67997a6f7ed349a883427c6423b3fc3d",
            "placeholder": "",
            "style": "IPY_MODEL_95a50b22991c408a90bafdfe3270075b",
            "value": "21282/21282[01:53&lt;00:00,723.91file/s]"
          }
        },
        "0508ebc903b34277bc2a3797c6a44a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0078471249bf40808d6e8a5e765a9f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a68ecaf65c411f9c16e57f18e90639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a71919d2157c40fc88f4a7fbc3506157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "14b235699da04c6b87ff0355e4f5bec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67997a6f7ed349a883427c6423b3fc3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a50b22991c408a90bafdfe3270075b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}